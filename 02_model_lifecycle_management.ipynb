{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97ff859-ed10-4645-8d40-842041ca7229",
   "metadata": {},
   "source": [
    "# Model Lifecycle Management\n",
    "\n",
    "In the previous chapter, we employed different ways of incorporating feedback from experts in our workflow, and evaluating it in ways that are aligned with business value. Now it is time for us to practice the skills needed to productize your model and ensure it continues to perform well thereafter by iteratively improving it. We will also learn to diagnose dataset shift and mitigate the effect that a changing environment can have on our model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1ee06e-f993-4f5c-84e0-8bfa6d700d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, roc_auc_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a5179a-a525-47ee-94e9-fdd9ce4806a0",
   "metadata": {},
   "source": [
    "## From workflows to pipelines\n",
    "\n",
    "Back in the arrhythmia startup, our monthly review is coming up, and as part of that an expert Python programmer will be reviewing our code. We decide to tidy up by following best practices and replace your script for feature selection and random forest classification, with a pipeline. We are using a training dataset available as `X_train` and `y_train` and a number of modules: `RandomForestClassifier`, `SelectKBest()` and `f_classif()` for feature selection, as well as `GridSearchCV` and `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee51d2e8-5360-4f62-b575-48b5521315d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrh = pd.read_csv('data/arrh.csv')\n",
    "# arrh['class'] = arrh['class'] == 'bad'\n",
    "X, y = arrh.drop('class', axis=1), arrh['class']\n",
    "\n",
    "# just to override the error of the SelectKBest \n",
    "X = X[ X.columns[X.std() > 2.1 ]]\n",
    "\n",
    "\n",
    "# Split the data into train and test, with 20% as test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# just to overaride the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e9c08b-29b5-4a16-a4b1-39454ac68ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 20}\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline with feature selector and classifier\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "params = {\n",
    "   'feature_selection__k':[10, 20],\n",
    "    'clf__n_estimators':[2, 5]}\n",
    "\n",
    "# Initialize the grid search object\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "# Fit it to the data and print the best value combination\n",
    "print(grid_search.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc24c3-0b9a-48ab-98ed-2acae09ff3b9",
   "metadata": {},
   "source": [
    "Wrapping up our workflow inside a pipeline is a sign of a true professional!\n",
    "\n",
    "We are proud of the improvement in our code quality, but just remembered that previously we had to use a custom scoring metric in order to account for the fact that false positives are costlier to our startup than false negatives. We hence want to equip your pipeline with scorers other than accuracy, including `roc_auc_score()`, `f1_score()`, and our own custom scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f4b937-e21f-4dd4-b068-b8bc4b327aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 20}\n"
     ]
    }
   ],
   "source": [
    "# Create a custom scorer\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Initialize the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ec9eec-4e22-4fc6-801b-f861294afb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 20}\n"
     ]
    }
   ],
   "source": [
    "# Create a custom scorer\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Initialise the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4c9a69-484b-49d6-a949-53a0d196060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 10}\n"
     ]
    }
   ],
   "source": [
    "def my_metric(y_test, y_est, cost_fp=10.0, cost_fn=1.0):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_est).ravel()\n",
    "    return cost_fp * fp + cost_fn * fn\n",
    "\n",
    "# Create a custom scorer\n",
    "scorer = make_scorer(my_metric)\n",
    "\n",
    "# Initialise the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536510a-63c2-4ebc-822b-d77ff573dd92",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can now incorporate the knowledge we acquired in Chapter 2 in our pipelines.\n",
    "\n",
    "## Model deployment\n",
    "\n",
    "Finally, it is time for us to push our first model to production. It is a random forest classifier which we will use as a baseline, while we are still working to develop a better alternative. We have access to the data split in training test with their usual names, `X_train`, `X_test`, `y_train` and `y_test`, as well as to the modules `RandomForestClassifier()` and `pickle`, whose methods `.load()` and `.dump()` we will need for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f86e6d9-0dbd-453f-9fec-82f1ef6bb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest to the training set\n",
    "clf = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# Save it to a file, to be pushed to production\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file=file)\n",
    "\n",
    "# Now load the model from file in the production environment\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    clf_from_file = pickle.load(file)\n",
    "\n",
    "# Predict the labels of the test dataset\n",
    "preds = clf_from_file.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33500d39-b481-4d51-a117-c648eba8be72",
   "metadata": {},
   "source": [
    "At some point, we were told that the sensors might be performing poorly for obese individuals. Previously we had dealt with that using weights, but now we are thinking that this information might also be useful for feature engineering, so we decide to replace the recorded weight of an individual with an indicator of whether they are obese. We want to do this using pipelines and available `FunctionTransformer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a8c271-6b3d-4e23-8624-3605408cdfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a feature extractor to flag very large values\n",
    "def more_than_average(X, multiplier=1.0):\n",
    "  Z = X.copy()\n",
    "  Z[:,1] = Z[:,1] > multiplier*np.mean(Z[:,1])\n",
    "  return Z\n",
    "\n",
    "# Convert your function so that it can be used in a pipeline\n",
    "pipe = Pipeline([\n",
    "  ('ft', FunctionTransformer(more_than_average)),\n",
    "  ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Optimize the parameter multiplier using GridSearchCV\n",
    "params = {'ft__multiplier': [1, 2, 3]}\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b72e15-a413-4b28-9cbd-b74d615e761e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Iterating without overfitting\n",
    "\n",
    "Having pushed our random forest to production, we suddenly worry that a naive Bayes classifier might be better. We want to run a champion-challenger test, by comparing a naive Bayes, acting as the challenger, to exactly the model which is currently in production, which we will load from file to make sure there is no confusion. We will use the F1 score for assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d696b68b-7a49-4220-811a-ce37c2a88cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8461538461538461\n",
      "0.8064516129032258\n"
     ]
    }
   ],
   "source": [
    "# Load the current model from disk\n",
    "champion = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "# Fit a Gaussian Naive Bayes to the training data\n",
    "challenger = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "# Print the F1 test scores of both champion and challenger\n",
    "print(f1_score(y_test, champion.predict(X_test)))\n",
    "print(f1_score(y_test, challenger.predict(X_test)))\n",
    "\n",
    "# Write back to disk the best-performing model\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(champion, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746368b8-0366-4002-91a1-f0c249ed51c7",
   "metadata": {},
   "source": [
    "This way of working is very similar to agile software development, and can greatly accelerate our workflows.\n",
    "\n",
    "We used grid search CV to tune our random forest classifier, and now want to inspect the cross-validation results to ensure we did not overfit. In particular we would like to take the difference of the mean test score for each fold from the mean training score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da697b62-1a1d-4642-8a5b-554040ec9b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.260235\n",
      "1   -0.184092\n",
      "2   -0.314394\n",
      "3   -0.256147\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline with feature selector and classifier\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "params = {\n",
    "   'feature_selection__k':[10, 20],\n",
    "    'clf__n_estimators':[2, 5]}\n",
    "\n",
    "\n",
    "# Fit your pipeline using GridSearchCV with three folds\n",
    "grid_search = GridSearchCV(pipe, params, cv=3, return_train_score=True)\n",
    "\n",
    "# Fit the grid search\n",
    "gs = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Store the results of CV into a pandas dataframe\n",
    "results = pd.DataFrame(gs.cv_results_)\n",
    "\n",
    "# Print the difference between mean test and training scores\n",
    "print(\n",
    "  results['mean_test_score']-results['mean_train_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be366b20-ca28-42b9-affb-707b7221818a",
   "metadata": {},
   "source": [
    "The difference between training and test performance seems quite big here, and that is always a telltale sign of overfitting!\n",
    "\n",
    "## Dataset shift\n",
    "\n",
    "We want to check for ourself that the optimal window size for the arrhythmia dataset is 50. We have been given the dataset as a pandas data frame called `arrh`, and want to use a subset of the data up to time `t_now`. Our test data is available as X_test, y_test. We will try out a number of window sizes, ranging from 10 to 100, fit a naive Bayes classifier to each window, assess its F1 score on the test data, and then pick the best performing window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67c4328-e426-41aa-bdb7-b3e132e12046",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrh = pd.read_csv('data/arrh.csv')\n",
    "# arrh['class'] = arrh['class'] == 'bad'\n",
    "X, y = arrh.drop('class', axis=1), arrh['class']\n",
    "\n",
    "# Split the data into train and test, with 20% as test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "t_now = 400\n",
    "accuracies = []\n",
    "wrange = range(10, 100, 10)\n",
    "\n",
    "# Loop over window sizes\n",
    "for w_size in wrange:\n",
    "\n",
    "    # Define sliding window\n",
    "    sliding = arrh.loc[(t_now - w_size + 1):t_now]\n",
    "\n",
    "    # Extract X and y from the sliding window\n",
    "    X, y = sliding.drop('class', axis=1), sliding['class']\n",
    "    \n",
    "    # Fit the classifier and store the F1 score\n",
    "    preds = GaussianNB().fit(X, y).predict(X_test)\n",
    "    accuracies.append(f1_score(y_test, preds))\n",
    "\n",
    "# Estimate the best performing window size\n",
    "optimal_window = wrange[np.argmax(accuracies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c2d80-c95a-49fc-8b5c-c0582a1eb67e",
   "metadata": {},
   "source": [
    "We now realise that the possibility of dataset shift introduces yet another parameter to optimize: the window size. This cannot be done with Cross-Validation on historical data, but instead requires the technique shown here.\n",
    "\n",
    "We have two concerns about our pipeline at the arrhythmia detection startup:\n",
    "\n",
    "- The app was trained on patients of all ages, but is primarily being used by fitness users who tend to be young. We suspect this might be a case of domain shift, and hence want to disregard all examples above 50 years old.\n",
    "- We are still concerned about overfitting, so we want to see if making the random forest classifier less complex and selecting some features might help with that.\n",
    "\n",
    "We will create a pipeline with a feature selection `SelectKBest()` step and a `RandomForestClassifier`. We also have access to `GridSearchCV()`, `Pipeline`, `numpy` as `np` and `pickle`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28a04c15-14d5-4386-bb61-9ea8976bd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "  ('ft', SelectKBest()), ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = {'ft__k':[5, 10], 'clf__max_depth':[10, 20]}\n",
    "\n",
    "# Execute grid search CV on a dataset containing under 50s\n",
    "grid_search = GridSearchCV(pipe, param_grid=grid)\n",
    "arrh = pd.read_csv('data/arrh.csv')\n",
    "arrh = arrh.loc[arrh['age'] < 50]\n",
    "# arrh['class'] = arrh['class'] == 'bad'\n",
    "X, y = arrh.drop('class', axis=1), arrh['class']\n",
    "\n",
    "# just to override the error of the SelectKBest \n",
    "X = X[ X.columns[X.std() > 2.25 ]]\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Push the fitted pipeline to production\n",
    "with open('pipe.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_search, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5d137-a843-4467-84cd-8bba2c2bad23",
   "metadata": {},
   "source": [
    "We are now an sklearn ninja and nothing can stop you. Except for ... a lack of labelled data! Let's see what we can do about that in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86614d-5c8b-44cc-8ace-e8753573a385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lingua",
   "language": "python",
   "name": "lingua"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
